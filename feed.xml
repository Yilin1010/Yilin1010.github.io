<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="Yilin1010.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="Yilin1010.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-09-21T04:23:53+00:00</updated><id>Yilin1010.github.io/feed.xml</id><title type="html">blank</title><subtitle>Yilin&apos;s interests and projects </subtitle><entry><title type="html">Review of Robustness of LLM unlearning</title><link href="Yilin1010.github.io/blog/2024/Robustness-LLM-Unlearning/" rel="alternate" type="text/html" title="Review of Robustness of LLM unlearning"/><published>2024-09-10T00:00:00+00:00</published><updated>2024-09-10T00:00:00+00:00</updated><id>Yilin1010.github.io/blog/2024/Robustness-LLM-Unlearning</id><content type="html" xml:base="Yilin1010.github.io/blog/2024/Robustness-LLM-Unlearning/"><![CDATA[<p><a href="https://docs.google.com/document/d/15xNESLFBRaIHTOUdp3VXqqceDSlLzJLQo91Fe7WS4xw/edit?usp=sharing">Google Doc</a></p> <p>Paper Review for two papers:</p> <ul> <li><a href="https://arxiv.org/abs/2404.15146">Rethinking LLM Memorization through the Lens of Adversarial Compression</a></li> <li><a href="https://arxiv.org/abs/2406.13356">JOGGING THE MEMORY OF UNLEARNED MODELS THROUGH TARGETED RELEARNING ATTACKS</a></li> </ul>]]></content><author><name></name></author><category term="posts"/><category term="Machine Unlearning"/><category term="Paper Review"/><category term="LLM"/><summary type="html"><![CDATA[Review of Robustness of LLM unlearning, LLM Memorization Metric, Adverserial Attack for MU]]></summary></entry></feed>