<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="Yilin1010.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="Yilin1010.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-11-05T10:08:56+00:00</updated><id>Yilin1010.github.io/feed.xml</id><title type="html">blank</title><subtitle>Yilin&apos;s interests and projects </subtitle><entry><title type="html">AI Safety, Software Engineering &amp;amp; HCI: Three Parts of My Experiences and How They Connect</title><link href="Yilin1010.github.io/blog/2024/Three-Parts-of-My-Experiences/" rel="alternate" type="text/html" title="AI Safety, Software Engineering &amp;amp; HCI: Three Parts of My Experiences and How They Connect"/><published>2024-11-05T00:00:00+00:00</published><updated>2024-11-05T00:00:00+00:00</updated><id>Yilin1010.github.io/blog/2024/Three-Parts-of-My-Experiences</id><content type="html" xml:base="Yilin1010.github.io/blog/2024/Three-Parts-of-My-Experiences/"><![CDATA[<h5 id="ai-safety-software-engineering--hci-three-parts-of-my-experiences-and-how-they-connect">AI Safety, Software Engineering &amp; HCI: Three Parts of My Experiences and How They Connect</h5> <h6 id="three-parts-of-my-experience">Three Parts of My Experience</h6> <p>I’d like to break down how my experiences and skills connect to this role in three main areas: <strong>AI safety research, software engineering practices, and user-centric design</strong></p> <p><strong>AI Safety Research:</strong> AI safety is a significant focus of my experience. From 4 years ago, I started reading papers and blogs in this field, I’ve been familiar with reproducing methods and customizing features for related AI algorithms.</p> <ul> <li>You can view my previous projects (machine unlearning algorithms and pipelines) on this repo and slide: <a href="https://github.com/Yilin1010/Selective-Gradient-Unlearning-SGU">Selective-Gradient-Unlearning-SGU</a>  <a href="https://github.com/Yilin1010/Machine_Unlearning_Algorithms/blob/master/slides_En.ipynb">[Slide]</a></li> </ul> <p><strong>Software Engineering Practices</strong>: I have a bachelor’s in software engineering, where I learned agile techniques. Later, I took a graduate course taught by a principal software engineer from Microsoft, and that’s where I got a lot of hands-on experience writing production-level code.</p> <p><strong>User Centric Design</strong>: I took a graduate-level HCI course taught by a former senior HCI scientist from Allen Institute for AI(AI2). My motivation for studying HCI stems from my curiosity about how large LLMs will transform human interact with devices. I wanted understand it better. This course has provided me with a structured HCI perspective. For our project, my team did user research and made a few small adjustments to ChatGPT’s website to improve trust and easier to use.”</p> <h6 id="how-these-experiences-connecting">How These Experiences Connecting?</h6> <p>My experience might seem broad, and someone maybe wonder it lacks focus. But after working on different projects, I realized that I’m really interested in one core idea: making practical adjustment to current solutions that improve user experience.</p> <p>My motivation comes from wanting to make users more satisfied than they are now.</p> <p>I think we don’t always have to completely reinvent solutions, I think it’s important to understand target users and the limitations of current methods before doing research or develop a products. So that we can make meaningful improvements without without having to replace everything that’s already working.</p>]]></content><author><name></name></author><category term="posts"/><category term="job"/><summary type="html"><![CDATA[AI Safety, Software Engineering &amp; HCI: Three Parts of My Experiences and How They Connect Three Parts of My Experience]]></summary></entry><entry><title type="html">Review of Robustness of LLM unlearning</title><link href="Yilin1010.github.io/blog/2024/Robustness-LLM-Unlearning/" rel="alternate" type="text/html" title="Review of Robustness of LLM unlearning"/><published>2024-09-10T00:00:00+00:00</published><updated>2024-09-10T00:00:00+00:00</updated><id>Yilin1010.github.io/blog/2024/Robustness-LLM-Unlearning</id><content type="html" xml:base="Yilin1010.github.io/blog/2024/Robustness-LLM-Unlearning/"><![CDATA[<p><a href="https://docs.google.com/document/d/15xNESLFBRaIHTOUdp3VXqqceDSlLzJLQo91Fe7WS4xw/edit?usp=sharing">Google Doc</a></p> <p>Paper Review for two papers:</p> <ul> <li><a href="https://arxiv.org/abs/2404.15146">Rethinking LLM Memorization through the Lens of Adversarial Compression</a></li> <li><a href="https://arxiv.org/abs/2406.13356">JOGGING THE MEMORY OF UNLEARNED MODELS THROUGH TARGETED RELEARNING ATTACKS</a></li> </ul>]]></content><author><name></name></author><category term="posts"/><category term="Machine Unlearning"/><category term="Paper Review"/><category term="LLM"/><summary type="html"><![CDATA[Review of Robustness of LLM unlearning, LLM Memorization Metric, Adverserial Attack for MU]]></summary></entry></feed>